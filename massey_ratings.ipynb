{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Massey Ratings\n",
    "    - Matt Robinson, Yale Undergraduate Sports Analytics Group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is one of several notebooks exploring ratings systems often used in sports. All of the notebooks can be found in [this repo](https://github.com/mc-robinson/Ratings_Methods). \n",
    "\n",
    "Specifically, this notebook attempts to both explain and implement the popular Massey ratings model created by [Ken Massey](http://www.masseyratings.com/). Legend has it that Massey, now a math professor, came up with this method while still an undergraduate (which I include as a fun fact and as motivation for me to figure out my life)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Basic Overview ##\n",
    "\n",
    "Massey's method is also referred to as the *Point Spread Method*. To put it simply, the goal of this method is for the difference in ratings between two teams to equal the difference in score when they play each other. Mathematically, this can be easily summarized by the equation:\n",
    "$$ r_i - r_j = y_k$$\n",
    "where $r_i$ and $r_j$ are the ratings of teams $i$ and $j$, respectively, and $y_k$ is the point differential for game $k$, in which $i$ and $j$ play each other.\n",
    "\n",
    "Now, of course, this equation is just the ideal. Over the course of a few games, you will get a system of equations that look just like it. Finding ratings such that it holds true for every team and every game is unrealistic. Therefore, Massey's method uses what is called the *method of least squares* to find the best approximate ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Working Through an Example ##\n",
    "\n",
    "In order to work through the implementation of this method and understand how it works, I'm going to use an extremely simplified example. Let's imagine the Ivy league is instead the IV league and consists of only four teams who each play each other once: Harvard, Princeton, Yale, and Columbia.\n",
    "\n",
    "Here are the results from the 2016 IV season that I scraped from the NCAA stats website:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "IV_df = pd.read_csv('IV_league_2016.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>team</th>\n",
       "      <th>opponent</th>\n",
       "      <th>location</th>\n",
       "      <th>team_score</th>\n",
       "      <th>opponent_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>Columbia</td>\n",
       "      <td>Princeton</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016</td>\n",
       "      <td>10</td>\n",
       "      <td>22</td>\n",
       "      <td>Harvard</td>\n",
       "      <td>Princeton</td>\n",
       "      <td>-1</td>\n",
       "      <td>23</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016</td>\n",
       "      <td>10</td>\n",
       "      <td>28</td>\n",
       "      <td>Columbia</td>\n",
       "      <td>Yale</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>Columbia</td>\n",
       "      <td>Harvard</td>\n",
       "      <td>-1</td>\n",
       "      <td>21</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>Princeton</td>\n",
       "      <td>Yale</td>\n",
       "      <td>-1</td>\n",
       "      <td>31</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2016</td>\n",
       "      <td>11</td>\n",
       "      <td>19</td>\n",
       "      <td>Harvard</td>\n",
       "      <td>Yale</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  month  day       team   opponent  location  team_score  \\\n",
       "0  2016     10    1   Columbia  Princeton         1          13   \n",
       "1  2016     10   22    Harvard  Princeton        -1          23   \n",
       "2  2016     10   28   Columbia       Yale         1          23   \n",
       "3  2016     11    5   Columbia    Harvard        -1          21   \n",
       "4  2016     11   12  Princeton       Yale        -1          31   \n",
       "5  2016     11   19    Harvard       Yale         1          14   \n",
       "\n",
       "   opponent_score  \n",
       "0              48  \n",
       "1              20  \n",
       "2              31  \n",
       "3              28  \n",
       "4               3  \n",
       "5              21  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IV_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `location` column tells us if the team in the `team` column was home or away. \n",
    "* Away = -1\n",
    "* Neutral = 0\n",
    "* Home = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the goal of Massey's method is to find ratings whose difference gives the score differential, we need to add a column with this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IV_df['score_diff'] = (IV_df['team_score']-IV_df['opponent_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>team</th>\n",
       "      <th>opponent</th>\n",
       "      <th>location</th>\n",
       "      <th>team_score</th>\n",
       "      <th>opponent_score</th>\n",
       "      <th>score_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>Columbia</td>\n",
       "      <td>Princeton</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>48</td>\n",
       "      <td>-35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016</td>\n",
       "      <td>10</td>\n",
       "      <td>22</td>\n",
       "      <td>Harvard</td>\n",
       "      <td>Princeton</td>\n",
       "      <td>-1</td>\n",
       "      <td>23</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016</td>\n",
       "      <td>10</td>\n",
       "      <td>28</td>\n",
       "      <td>Columbia</td>\n",
       "      <td>Yale</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>31</td>\n",
       "      <td>-8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>Columbia</td>\n",
       "      <td>Harvard</td>\n",
       "      <td>-1</td>\n",
       "      <td>21</td>\n",
       "      <td>28</td>\n",
       "      <td>-7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>Princeton</td>\n",
       "      <td>Yale</td>\n",
       "      <td>-1</td>\n",
       "      <td>31</td>\n",
       "      <td>3</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2016</td>\n",
       "      <td>11</td>\n",
       "      <td>19</td>\n",
       "      <td>Harvard</td>\n",
       "      <td>Yale</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>21</td>\n",
       "      <td>-7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  month  day       team   opponent  location  team_score  \\\n",
       "0  2016     10    1   Columbia  Princeton         1          13   \n",
       "1  2016     10   22    Harvard  Princeton        -1          23   \n",
       "2  2016     10   28   Columbia       Yale         1          23   \n",
       "3  2016     11    5   Columbia    Harvard        -1          21   \n",
       "4  2016     11   12  Princeton       Yale        -1          31   \n",
       "5  2016     11   19    Harvard       Yale         1          14   \n",
       "\n",
       "   opponent_score  score_diff  \n",
       "0              48         -35  \n",
       "1              20           3  \n",
       "2              31          -8  \n",
       "3              28          -7  \n",
       "4               3          28  \n",
       "5              21          -7  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IV_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Setup ###\n",
    "\n",
    "Now comes time to get back to our original equation:\n",
    "$$ r_i - r_j = y_k $$\n",
    "\n",
    "In this case, we are going to try to find the ratings for all four teams of the IV league: $r_h, r_p, r_y,$ and $r_c$. To do this, we look at the six league games from the IV season and construct an equation for each game. In our case, the system of equations looks like this (using the data from the table above):\n",
    "\n",
    "$$ \n",
    "\\begin{align*}\n",
    "r_c - r_p &= -35 \\\\\n",
    "r_h - r_p &=  3 \\\\\n",
    "r_c - r_y &= -8 \\\\\n",
    "r_c - r_h &= -7 \\\\\n",
    "r_p - r_y &= 28 \\\\\n",
    "r_h - r_y &= -7 \\\\\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For sake of simplicity, I am going to take every equation with a negative `score_diff` and multiply through by $-1$. Now every point differential is positive and the system of equations looks like this:\n",
    "$$ \n",
    "\\begin{align*}\n",
    "r_p - r_c &= 35 \\\\\n",
    "r_h - r_p &=  3 \\\\\n",
    "r_y - r_c &= 8 \\\\\n",
    "r_h - r_c &= 7 \\\\\n",
    "r_p - r_y &= 28 \\\\\n",
    "r_y - r_h &= 7 \\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now encode this system of equations in matrix form. Generally, in a season of $m$ games among $n$ different teams, there will be $m$ equations (one for each game) and $n$ unknowns (the ratings). Thus, we can create an $m \\times n$ matrix to encode the equations from the $m$ games and $n$ teams. For our example with six games and four teams, the matrix $X$ looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  0, -1],\n",
       "       [ 1, -1,  0,  0],\n",
       "       [ 0,  0,  1, -1],\n",
       "       [ 1,  0,  0, -1],\n",
       "       [ 0,  1, -1,  0],\n",
       "       [-1,  0,  1,  0]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([\n",
    "        [0, 1, 0, -1],\n",
    "        [1, -1, 0, 0],\n",
    "        [0, 0, 1, -1],\n",
    "        [1, 0, 0, -1],\n",
    "        [0, 1, -1, 0],\n",
    "        [-1, 0, 1, 0]\n",
    "    ])\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where each of the columns corresponds to a team, and each row is a game. The labeled output may be more clear:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Harvard  Princeton  Yale  Columbia\n",
      "game_1        0          1     0        -1\n",
      "game_2        1         -1     0         0\n",
      "game_3        0          0     1        -1\n",
      "game_4        1          0     0        -1\n",
      "game_5        0          1    -1         0\n",
      "game_6       -1          0     1         0\n"
     ]
    }
   ],
   "source": [
    "game_num = ['game_' + _ for _ in '123456']\n",
    "team_names = ['Harvard', 'Princeton', 'Yale', 'Columbia']\n",
    "X_df = pd.DataFrame(X, index=game_num, columns=team_names)\n",
    "print(X_df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the first row corresponds to the game between Princeton and Columbia. The winning team, Princeton, gets a $1$ in its column while the losing team, Columbia, gets a $-1$ in its column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our full system of equations can be written using the linear matrix equation $Xr=y$, where $X$ is the $m \\times n$ matrix we just constructed, $r$ is the $n \\times 1$ vector containing the ratings of each team, and $y$ is the $m \\times 1$ vector of score differences from each game.\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "  0 & 1 & 0 & -1 \\\\\n",
    "  1 & -1 & 0 & 0 \\\\\n",
    "  0 & 0 & 1 & -1 \\\\\n",
    "  1 & 0 & 0 & -1 \\\\\n",
    "  0 & 1 & -1 & 0 \\\\\n",
    "  -1 & 0 & 1 & 0\n",
    "\\end{bmatrix} \n",
    "\\begin{bmatrix}\n",
    "  r_h \\\\\n",
    "  r_p \\\\\n",
    "  r_y \\\\\n",
    "  r_c\n",
    "\\end{bmatrix}\n",
    "= \n",
    "\\begin{bmatrix}\n",
    "  35 \\\\\n",
    "  3 \\\\\n",
    "  8 \\\\\n",
    "  7 \\\\\n",
    "  28 \\\\\n",
    "  7\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Now, you can see that it is pretty easy to construct this matrix equation from the beginning. For each game (row), just put a $1$ in the column of the winning team, a $-1$ in the column of the losing team, and encode the absolute value of the point differential (the margin of victory) in the $y$ vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving for the Ratings ###\n",
    "\n",
    "In any given season, it is likely that the number of games played ($m$) will be much greater than the number of teams ($n$). This means that we have more equations than unknowns. In our IV league example, we had six equations but only four unknowns. Mathematically, this means that the matrix is *overdetermined*, and thus the system of equations is likely *inconsistent*. In simpler words, this means that there is almost certainly no solution to the system of equations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unnecsary Aside: A simple example for those unfamiliar with least squares ####\n",
    "Take this really simple example of two equations and one unknown. \n",
    "$$\n",
    "\\begin{align*}\n",
    "x &= 5 \\\\\n",
    "x &= 9\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Obviously, there is no $x$ that satisfies both of these equations. So we are going to need to look for the best approximate solution $\\bar x$. But how do we measure which is the \"best\" solution?\n",
    "\n",
    "Often, we consider the best fit to be the one that minimizes the *sum of the squared errors*. To be more mathematical, for our system of $m=2$ and $n=1$ unknowns, we seek the $\\hat x$ that minimizes the following:\n",
    "$$\n",
    "\\sum_{i=1}^m (x-\\bar x)^2 = (9-\\bar x)^2 + (5- \\bar x)^2\n",
    "$$\n",
    "Now I think it is intuitive that the solution is somewhere between $5$ and $9$. The first guess is probably the average of $5$ and $9$, $\\bar x = 7$ But some of you may think that it would be more advantageous to choose $\\bar x = 6$ or  $\\bar x = 8$ since they are closer to one of the numbers, or perhaps even $\\bar x = 5$ or  $\\bar x = 9$ since it makes one of the eqations exactly true. Well, it turns out that the average $\\bar x = 7$ is the best choice because it minimizes $\\sum_{i=1}^m (x-\\bar x)^2$ if you do the math out. (This is no coincidence; the *mean* is precisely that estimator that minimizes the sum of squared errors)  \n",
    "\n",
    "So this is exactly what we are trying to with our matrix equation: find those ratings that give the best approximate answer to our system of equations, as determined by the least squares criterion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The least squares solution to our matrix equation #### \n",
    "\n",
    "So in the case of our matrix equation $Xr=y$ that we cannot find a solution for, we seek the least squares solution $\\hat r$ such that the error $E = \\parallel{Xr - y}\\parallel^2$ is minimized.  \n",
    "\n",
    "To find this $\\hat r$ we focus on solving what are called the \"normal equations,\" $X^TX\\hat r = X^T y$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unnecessary Aside: Deriving the Normal Equations ####\n",
    "\n",
    "In the machine learning courses I have seen, the normal equations are often derived using calculus. However, I find matrix calculus a bit narly, so instead I am going to use some geometry to derive them. If you want to see the calculus derivation, please see a linear algebra/machine learning textbook.\n",
    "\n",
    "When we are trying to solve $Xr=y$, a solution exists if and only if a linear combination of the columns of $X$ can equal the $y$ vector. That is to say (in slightly more mathematical jargon), a solution exists if $y$ lies in the column space of $X$, which I'll call $C(X)$. Well since we have an overdetermined system, $y$ is almost definitely outside of $C(X)$. So what do we do? The answer is that we give up on solving exactly for $y$. Instead, let's look for $X\\hat r$, the vector in $C(X)$ that is as close as possible to $y$. \n",
    "\n",
    "But what does close mean? It means that the error vector $e = (y - X \\hat r)$ is perpendicular to the vector $X \\hat r$, which is in $C(X)$. (If the reason for this is not clear to you, this [medium post](https://medium.com/@andrew.chamberlain/the-linear-algebra-view-of-least-squares-regression-f67044b7f39b) has a rather good illustration)\n",
    "\n",
    "And when two vectors are perpendicular, we know that their dot product must be $0$. So let's do it out and solve for the approximate ratings vector $\\hat r$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "(X \\hat r)^T (y - X \\hat r) &= 0 \\\\\n",
    "\\hat r^T X^T y -\\hat r^T X^T X \\hat r &= 0 \\\\\n",
    "\\hat r^T X^T X \\hat r &= \\hat r^T X^T y \\\\\n",
    "(\\hat r^T)^{-1} \\hat r^T X^T X \\hat r &= (\\hat r^T)^{-1} \\hat r^T X^T y \\\\\n",
    "X^T X \\hat r &= X^T y \\\\ \n",
    "\\end{align*}\n",
    "$$\n",
    "And, if $X^T X$ is invertible:\n",
    "$$\n",
    "\\hat r = (X^T X)^{-1} X^T y \n",
    "$$\n",
    "\n",
    "If anyone cares, the matrix $(X^T X)^{-1} X^T$ is often referred to as the *Moore-Penrose Pseudoinverse*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Back to the example\n",
    "\n",
    "Anyways, let's do the math out for our IV league example and attempt to solve the normal equations $X^TX\\hat r = X^T y$:\n",
    "\n",
    "First examine what the matrix product $X^TX$ looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X looks like:\n",
      "        Harvard  Princeton  Yale  Columbia\n",
      "game_1        0          1     0        -1\n",
      "game_2        1         -1     0         0\n",
      "game_3        0          0     1        -1\n",
      "game_4        1          0     0        -1\n",
      "game_5        0          1    -1         0\n",
      "game_6       -1          0     1         0\n",
      "\n",
      "X^T looks like:\n",
      "           game_1  game_2  game_3  game_4  game_5  game_6\n",
      "Harvard         0       1       0       1       0      -1\n",
      "Princeton       1      -1       0       0       1       0\n",
      "Yale            0       0       1       0      -1       1\n",
      "Columbia       -1       0      -1      -1       0       0\n"
     ]
    }
   ],
   "source": [
    "print(\"X looks like:\")\n",
    "print(X_df.to_string() + '\\n')\n",
    "print(\"X^T looks like:\")\n",
    "print(X_df.T.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's call the matrix product $M$ for short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3, -1, -1, -1],\n",
       "       [-1,  3, -1, -1],\n",
       "       [-1, -1,  3, -1],\n",
       "       [-1, -1, -1,  3]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M = (X.T).dot(X)\n",
    "M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Showing the labels that each row and column correspond to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Harvard  Princeton  Yale  Columbia\n",
      "Harvard          3         -1    -1        -1\n",
      "Princeton       -1          3    -1        -1\n",
      "Yale            -1         -1     3        -1\n",
      "Columbia        -1         -1    -1         3\n"
     ]
    }
   ],
   "source": [
    "team_names = ['Harvard', 'Princeton', 'Yale', 'Columbia']\n",
    "M_df = pd.DataFrame(M, index=team_names, columns=team_names)\n",
    "print(M_df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may also help to see the whole multiplication written out:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "M = X^TX =\n",
    "\\begin{bmatrix}\n",
    "  0 & 1 & 0 & 1 & 0 & -1 \\\\\n",
    "  1 & -1 & 0 & 0 & 1 & 0 \\\\\n",
    "  0 & 0 & 1 & 0 & -1 & 1 \\\\\n",
    "  -1 & 0 & -1 & -1 & 0 & 0\n",
    "\\end{bmatrix} \n",
    "\\begin{bmatrix}\n",
    "  0 & 1 & 0 & -1 \\\\\n",
    "  1 & -1 & 0 & 0 \\\\\n",
    "  0 & 0 & 1 & -1 \\\\\n",
    "  1 & 0 & 0 & -1 \\\\\n",
    "  0 & 1 & -1 & 0 \\\\\n",
    "  -1 & 0 & 1 & 0\n",
    "\\end{bmatrix} \n",
    "=\n",
    "\\begin{bmatrix}\n",
    "  3 & -1 & -1 & -1 \\\\\n",
    "  -1 & 3 & -1 & -1 \\\\\n",
    "  -1 & -1 & 3 & -1 \\\\\n",
    "  -1 & -1 & -1 & 3\n",
    "\\end{bmatrix}  \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$M$ is an $n \\times n$ matrix. In general, the diagonal entries $M_{ii}$ correspond to the total number of games played by team $i$. The other entries $M_{ij}$ for $i \\neq j$ are simply equal to the negation of how many times team $i$ played team $j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's move on to the right side of the equation, $X^T y$. For sale of simplicity, we will call this vector $p$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[35],\n",
       "       [ 3],\n",
       "       [ 8],\n",
       "       [ 7],\n",
       "       [28],\n",
       "       [ 7]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yT = np.array([\n",
    "        [35, 3, 8, 7, 28, 7]\n",
    "    ])\n",
    "y = yT.T\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  3],\n",
       "       [ 60],\n",
       "       [-13],\n",
       "       [-50]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = (X.T).dot(y)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             \n",
      "Harvard     3\n",
      "Princeton  60\n",
      "Yale      -13\n",
      "Columbia  -50\n"
     ]
    }
   ],
   "source": [
    "team_names = ['Harvard', 'Princeton', 'Yale', 'Columbia']\n",
    "p_df = pd.DataFrame(p, index=team_names,columns=[''])\n",
    "print(p_df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of you may notice something special about this vector $p$. I'll write the multiplication out in full so that it may be more clear to you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "p = X^Ty =\n",
    "\\begin{bmatrix}\n",
    "  0 & 1 & 0 & 1 & 0 & -1 \\\\\n",
    "  1 & -1 & 0 & 0 & 1 & 0 \\\\\n",
    "  0 & 0 & 1 & 0 & -1 & 1 \\\\\n",
    "  -1 & 0 & -1 & -1 & 0 & 0\n",
    "\\end{bmatrix} \n",
    "\\begin{bmatrix}\n",
    "  35 \\\\\n",
    "  3 \\\\\n",
    "  8 \\\\\n",
    "  7 \\\\\n",
    "  28 \\\\\n",
    "  7\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "  3 \\\\\n",
    "  60 \\\\\n",
    "  -13 \\\\\n",
    "  -50 \n",
    "\\end{bmatrix} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vector $p$ encodes the total point differential, summed across every game for a given team. Let's just examine the first row of $p$, which encodes Harvard's total point differential. \n",
    "\n",
    "Harvard played three games in the IV league. In their first game they beat Princeton with a point differential of +3. In their second game, they beat Columbia with a point differential of +7. In their last game, they lost to Yale by a point differential of -7. Alltogether, their total point is 7 + 3 -7 = 3.\n",
    "\n",
    "If you do the matrix multiplaction for the first row, the same exact thing is happening. You add all the point differentials from Harvard's win and subtract the point differentials from the loss. Thus, $p$ simply encodes the cumulative point differentials for each team.\n",
    "\n",
    "Now, it is also pretty easy to see that both $M$ and $p$ can be constructed pretty simply from the beginning without having to first construct $X$ and $y$. This avoids some unnecessary computation. Also note that $M$ is $n \\times n$ and is thus ususally much smaller than the $m \\times n$ matrix $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finally Solving the Normal Equations.\n",
    "\n",
    "Now that we have seen how to construct $M$ and $p$, let's solve for $\\hat r$ using the normal equations $M \\hat r = p$.\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "  3 & -1 & -1 & -1 \\\\\n",
    "  -1 & 3 & -1 & -1 \\\\\n",
    "  -1 & -1 & 3 & -1 \\\\\n",
    "  -1 & -1 & -1 & 3\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "  r_h \\\\\n",
    "  r_p \\\\\n",
    "  r_y \\\\\n",
    "  r_c\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "  3 \\\\\n",
    "  60 \\\\\n",
    "  -13 \\\\\n",
    "  -50 \n",
    "\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "(Note: I probably should put hats over the individual ratings in $\\hat r$, but by now I think it is clear that these ratings will be an approximation.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But before we solve for the ratings, it is worth also examining the columns of $M$ to spot **one minor issue**. Some of you may have seen immedietly this interesting property of the columns of $M$:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "   3 \\\\\n",
    "  -1 \\\\\n",
    "  -1 \\\\\n",
    "  -1 \n",
    "\\end{bmatrix}  \n",
    "+ \n",
    "\\begin{bmatrix}\n",
    "  -1 \\\\\n",
    "  3 \\\\\n",
    "  -1 \\\\\n",
    "  -1 \n",
    "\\end{bmatrix} \n",
    "+\n",
    "\\begin{bmatrix}\n",
    "  -1 \\\\\n",
    "  -1 \\\\\n",
    "  3 \\\\\n",
    "  -1 \n",
    "\\end{bmatrix} \n",
    "+ \n",
    "\\begin{bmatrix}\n",
    "  -1 \\\\\n",
    "  -1 \\\\\n",
    "  -1 \\\\\n",
    "  3 \n",
    "\\end{bmatrix} \n",
    "= \n",
    "\\begin{bmatrix}\n",
    "  0 \\\\\n",
    "  0 \\\\\n",
    "  0 \\\\\n",
    "  0 \n",
    "\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "Some of you will also remember that a set of vectors is *linearly dependent* if one of the vectors can be written as a linear combination of the others vectors. Well let me just move one of the vectors over to the other side and multiply the equation by $-1$:\n",
    "\n",
    "$$\n",
    "-1*\n",
    "\\begin{bmatrix}\n",
    "   3 \\\\\n",
    "  -1 \\\\\n",
    "  -1 \\\\\n",
    "  -1 \n",
    "\\end{bmatrix}  \n",
    "+ \n",
    "-1*\n",
    "\\begin{bmatrix}\n",
    "  -1 \\\\\n",
    "  3 \\\\\n",
    "  -1 \\\\\n",
    "  -1 \n",
    "\\end{bmatrix} \n",
    "+\n",
    "-1*\n",
    "\\begin{bmatrix}\n",
    "  -1 \\\\\n",
    "  -1 \\\\\n",
    "  3 \\\\\n",
    "  -1 \n",
    "\\end{bmatrix} \n",
    "= \n",
    "\\begin{bmatrix}\n",
    "  -1 \\\\\n",
    "  -1 \\\\\n",
    "  -1 \\\\\n",
    "  3 \n",
    "\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "It's now pretty obvious that this set of vectors is linearly dependent. You may similarly notice that the same is true of the rows of $M$. These properties also hold true in general when we consider the matrix $M$ over an arbitrary set of games and teams.\n",
    "\n",
    "The linear dependence of the columns of $M$ has some interesting properties that we need to consider. The first thing you may remember from a linear algebra class is that linearly dependent columns is one of the many, many equivalent ways to say that a matrix is not invertible. Also, we observe that $rank(M) < n$, and thus the solution for $\\hat r$ is not unique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Massey came up with a **clever fix** for this. He simply replaces the last row of $M$ with a row of all ones. and sets the last entry of $p$ to be $0$. This new system is now denoted $\\bar M \\hat r = \\bar p$ and looks like this:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "  3 & -1 & -1 & -1 \\\\\n",
    "  -1 & 3 & -1 & -1 \\\\\n",
    "  -1 & -1 & 3 & -1 \\\\\n",
    "  1 & 1 & 1 & 1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "  r_h \\\\\n",
    "  r_p \\\\\n",
    "  r_y \\\\\n",
    "  r_c\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "  3 \\\\\n",
    "  60 \\\\\n",
    "  -13 \\\\\n",
    "  0 \n",
    "\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "$\\bar M$ is now full rank and there is a unique solution for $\\hat r$. There is one other nice property of this new setup which can be seen by doing the matrix multiplication for the last row. \n",
    "$$\n",
    "1*r_h + 1*r_p + 1*r_y + 1*r_c = 0\n",
    "$$\n",
    "That is, all of the rankings must now sum to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now finally get a solution using numpy for the ratings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3, -1, -1, -1],\n",
       "       [-1,  3, -1, -1],\n",
       "       [-1, -1,  3, -1],\n",
       "       [ 1,  1,  1,  1]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M_bar = M.copy()\n",
    "M_bar[-1,:] = np.ones(M.shape[0])\n",
    "M_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  3],\n",
       "       [ 60],\n",
       "       [-13],\n",
       "       [  0]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_bar = p\n",
    "p_bar[-1] = 0\n",
    "p_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.75],\n",
       "       [ 15.  ],\n",
       "       [ -3.25],\n",
       "       [-12.5 ]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_hat = np.linalg.inv(M_bar).dot(p_bar)\n",
    "r_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                \n",
      "Harvard     0.75\n",
      "Princeton  15.00\n",
      "Yale       -3.25\n",
      "Columbia  -12.50\n"
     ]
    }
   ],
   "source": [
    "team_names = ['Harvard', 'Princeton', 'Yale', 'Columbia']\n",
    "r_df = pd.DataFrame(r_hat, index=team_names,columns=[''])\n",
    "print(r_df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Additional Features\n",
    "\n",
    "### Offense and Defense Vectors\n",
    "\n",
    "One of the coolest features of Massey's method is the ability to split the overall ratings vector $r$ into an offensive ratings vector $o$ and a defensive ratings vector $d$. Massey's simple assumption is that the each team rating $r_i$ can be considered the sum of its offensive and defensive ratings:\n",
    "$$\n",
    "r_i = o_i + d_i\n",
    "$$\n",
    "\n",
    "In our example, the vector $r=o+d$ looks as follows:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "  r_h \\\\\n",
    "  r_p \\\\\n",
    "  r_y \\\\\n",
    "  r_c\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "  o_h \\\\\n",
    "  o_p \\\\\n",
    "  o_y \\\\\n",
    "  o_c\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "  d_h \\\\\n",
    "  d_p \\\\\n",
    "  d_y \\\\\n",
    "  d_c\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "To solve for these offensive and defensive ratings, we also need to do a few things to the right hand side of the equation $Mr = p$. As we know from before, $p$ is the vector of cumulative point differentials $p_i$ for each team $i$. We can decompose each $p_i$ into the total points scored by team $i$ minus the total points scored against $i$. In vector form, we set $p=f-a$, where $p$ is the 'points for\" vector, and $a$ is the points against vector. \n",
    "\n",
    "In our IV example, $p=f-a$ is decomposed as follows:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "  3 \\\\\n",
    "  60 \\\\\n",
    "  -13 \\\\\n",
    "  -50 \n",
    "\\end{bmatrix} \n",
    "= \n",
    "\\begin{bmatrix}\n",
    "  65 \\\\\n",
    "  99 \\\\\n",
    "  55 \\\\\n",
    "  57 \n",
    "\\end{bmatrix} \n",
    "-\n",
    "\\begin{bmatrix}\n",
    "  62 \\\\\n",
    "  39 \\\\\n",
    "  68 \\\\\n",
    "  107 \n",
    "\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Before we can continue, we also need to decompose $M$, which we call the \"Massey coefficient matrix.\" We split $M$ into $M=T-P$, where $T$ is a diagonal matrix and $P$ is an off-diagonal matrix. $T$ holds the total number of games played by each team on the diagonal entries. $P$ meanwhile, holds the number of times two teams play each other in a season.\n",
    "\n",
    "$M=T-P$ for our IV example looks as follows:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "  3 & -1 & -1 & -1 \\\\\n",
    "  -1 & 3 & -1 & -1 \\\\\n",
    "  -1 & -1 & 3 & -1 \\\\\n",
    "  -1 & -1 & -1 & 3\n",
    "\\end{bmatrix}\n",
    "= \n",
    "\\begin{bmatrix}\n",
    "  3 & 0 & 0 & 0 \\\\\n",
    "  0 & 3 & 0 & 0 \\\\\n",
    "  0 & 0 & 3 & 0 \\\\\n",
    "  0 & 0 & 0 & 3\n",
    "\\end{bmatrix}\n",
    "-\n",
    "\\begin{bmatrix}\n",
    "  0 & 1 & 1 & 1 \\\\\n",
    "  1 & 0 & 1 & 1 \\\\\n",
    "  1 & 1 & 0 & 1 \\\\\n",
    "  1 & 1 & 1 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We now work with the original system $Mr=p$ to derive the $o$ and $d$ vectors.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "Mr &= p \\\\\n",
    "(T-P)r &= p \\\\\n",
    "(T-P)(o+d) &= p \\\\\n",
    "To-Po+Td-Pd &= p \\\\\n",
    "To-Po+Td-Pd &= f-a\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "We split the final equation into two separate equations:\n",
    "\n",
    "$To-Pd=f$ and $Po-Td=a$\n",
    "\n",
    "Let's look at these equations in turn:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the left equation $To-Pd=f$:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "  3 & 0 & 0 & 0 \\\\\n",
    "  0 & 3 & 0 & 0 \\\\\n",
    "  0 & 0 & 3 & 0 \\\\\n",
    "  0 & 0 & 0 & 3\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "  o_h \\\\\n",
    "  o_p \\\\\n",
    "  o_y \\\\\n",
    "  o_c\n",
    "\\end{bmatrix}\n",
    "-\n",
    "\\begin{bmatrix}\n",
    "  0 & 1 & 1 & 1 \\\\\n",
    "  1 & 0 & 1 & 1 \\\\\n",
    "  1 & 1 & 0 & 1 \\\\\n",
    "  1 & 1 & 1 & 0\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "  d_h \\\\\n",
    "  d_p \\\\\n",
    "  d_y \\\\\n",
    "  d_c\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "  65 \\\\\n",
    "  99 \\\\\n",
    "  55 \\\\\n",
    "  57 \n",
    "\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "Let's just do the multiplication out:\n",
    "\n",
    "$$\n",
    "3o_h - d_p - d_y - d_c = 65 \\\\\n",
    "3o_p - d_h - d_y - d_c = 99 \\\\\n",
    "3o_y - d_h - d_p - d_c = 55 \\\\\n",
    "3o_c - d_h - d_p - d_y = 57\n",
    "$$\n",
    "\n",
    "All these equations say is that the total number of points scored by one team in a season should equal the number of games played times their offensive rating minus the defensive ratings for each of the teams they played. For example, the first row says that the 65 points Harvard scored throughout the IV season should equal 3 times their offensive ratings minus the sum of the defensive ratings of Princeton, Yale, and Columbia (their opponents in the 3 games).\n",
    "\n",
    "Let's work with this equation some more:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "To-Pd &= f \\\\\n",
    "T(r-d)-Pd &= f \\\\\n",
    "(T+P)d &= Tr-f\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The right hand side of this equation, $Tr-f$, can be computed quite easily since we already know $T$,$r$, and $f$. Thus we just have to solve this system to find $d$, then we can find $o$ from $o$ = $r$ - $d$.\n",
    "\n",
    "Let's solve it for our IV example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 0, 0, 0],\n",
       "       [0, 3, 0, 0],\n",
       "       [0, 0, 3, 0],\n",
       "       [0, 0, 0, 3]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T = np.diag(np.diag(M))\n",
    "T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1],\n",
       "       [1, 0, 1, 1],\n",
       "       [1, 1, 0, 1],\n",
       "       [1, 1, 1, 0]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P = T.copy() - M.copy()\n",
    "P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[65],\n",
       "       [99],\n",
       "       [55],\n",
       "       [57]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fT = np.array([\n",
    "        [65, 99, 55, 57]\n",
    "    ])\n",
    "f = fT.T\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -8.375],\n",
       "       [ -4.   ],\n",
       "       [ -9.375],\n",
       "       [-24.25 ]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = np.linalg.inv(T+P).dot(T.dot(r_hat)-f)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defensive ratings d_i for each team:\n",
      "                 \n",
      "Harvard    -8.375\n",
      "Princeton  -4.000\n",
      "Yale       -9.375\n",
      "Columbia  -24.250\n"
     ]
    }
   ],
   "source": [
    "team_names = ['Harvard', 'Princeton', 'Yale', 'Columbia']\n",
    "d_df = pd.DataFrame(d, index=team_names,columns=[''])\n",
    "print(\"defensive ratings d_i for each team:\")\n",
    "print(d_df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  9.125],\n",
       "       [ 19.   ],\n",
       "       [  6.125],\n",
       "       [ 11.75 ]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o = r_hat.copy() - d.copy()\n",
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "offensive ratings o_i for each team:\n",
      "                 \n",
      "Harvard     9.125\n",
      "Princeton  19.000\n",
      "Yale        6.125\n",
      "Columbia   11.750\n"
     ]
    }
   ],
   "source": [
    "team_names = ['Harvard', 'Princeton', 'Yale', 'Columbia']\n",
    "o_df = pd.DataFrame(o, index=team_names,columns=[''])\n",
    "print(\"offensive ratings o_i for each team:\")\n",
    "print(o_df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Massey considers these offensive ratings $o_i$ to be \"the number of points team $i$ would score against an average defense.\" Likewise, the defensive rating ?????$d_i$ for each team $i$ can be thought of as the number of points team $i$ would give up against an average offense. ??????\n",
    "\n",
    "Another nice thing about the vecotrs $o$ and $d$ is that we can use them to predict the point outcomes for each game. For example, let's take a theoretical matchup of Princeton vs. Harvard on a neutral field.\n",
    "\n",
    "To predict the point spread, I would usually just take the difference in the ratings for the two teams, $r_p$ and $r_h$:\n",
    "\n",
    "$$\n",
    "r_p - r_h = 15.00 - 0.75 = 14.25,\n",
    "$$\n",
    "\n",
    "which predicts that Princeton will win by about two touchdowns. But let's play with the equation a bit more:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "r_p - r_h &= (o_p + d_p) - (o_h + d_h) \\\\\n",
    "&= (o_p - d_h) - (o_h - d_p) \\\\\n",
    "&= (19.000 - (-8.375)) - (9.125 - (-4.000)) \\\\\n",
    "&= 27.375 - 13.125 \\\\\n",
    "&= 14.25,\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "which is the same answer we got before. However, in this case, we also get a prediction of the game score (27.375 to 13.125), not just the spread. This interpretation of the equation is nice because it gives us the predicted number of points each team will score against each other based on the strengths of their respective offenses and defenses. For example $o_p - d_h$ is the predicted number of points the Princeton offense will score on the Harvard defense. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Home-field Advantage \n",
    "\n",
    "It is quite easy to add in home-field advantage into the Massey ratings systems. We simply seek to find the value of some home advantage, which we call HA.\n",
    "\n",
    "Let's go back to our original system of equations:\n",
    "\n",
    "$$ \n",
    "\\begin{align*}\n",
    "r_p - r_c &= 35 \\\\\n",
    "r_h - r_p &=  3 \\\\\n",
    "r_y - r_c &= 8 \\\\\n",
    "r_h - r_c &= 7 \\\\\n",
    "r_p - r_y &= 28 \\\\\n",
    "r_y - r_h &= 7 \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "What we do is add HA to the left side of all equations in which the winning team was at home, and we subtract HA from the left side of those equations for which the winning team was away.\n",
    "\n",
    "The result looks like this:\n",
    "\n",
    "$$ \n",
    "\\begin{align*}\n",
    "r_p - r_c - HA &= 35 \\\\\n",
    "r_h - r_p - HA &=  3 \\\\\n",
    "r_y - r_c - HA &= 8 \\\\\n",
    "r_h - r_c + HA &= 7 \\\\\n",
    "r_p - r_y - HA &= 28 \\\\\n",
    "r_y - r_h - HA &= 7 \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "(Note: we are in the somewhat odd situation that the home team lost in 5 of the 6 league games).\n",
    "\n",
    "Encoding this in matrix form, we got the following system:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "  0 & 1 & 0 & -1 & -1 \\\\\n",
    "  1 & -1 & 0 & 0 & -1 \\\\\n",
    "  0 & 0 & 1 & -1 & -1 \\\\\n",
    "  1 & 0 & 0 & -1 & 1\\\\\n",
    "  0 & 1 & -1 & 0 & -1\\\\\n",
    "  -1 & 0 & 1 & 0 & 1\n",
    "\\end{bmatrix} \n",
    "\\begin{bmatrix}\n",
    "  r_h \\\\\n",
    "  r_p \\\\\n",
    "  r_y \\\\\n",
    "  r_c \\\\\n",
    "  HA\n",
    "\\end{bmatrix}\n",
    "= \n",
    "\\begin{bmatrix}\n",
    "  35 \\\\\n",
    "  3 \\\\\n",
    "  8 \\\\\n",
    "  7 \\\\\n",
    "  28 \\\\\n",
    "  7\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_HA = np.array([\n",
    "        [0, 1, 0, -1, -1],\n",
    "        [1, -1, 0, 0, -1],\n",
    "        [0, 0, 1, -1, -1],\n",
    "        [1, 0, 0, -1, 1],\n",
    "        [0, 1, -1, 0, -1],\n",
    "        [-1, 0, 1, 0, -1]\n",
    "    ])\n",
    "X_HA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r_HA = np.linalg.pinv(X_HA).dot(y)\n",
    "r_HA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "col_names = ['Harvard', 'Princeton', 'Yale', 'Columbia','location']\n",
    "r_HA_df = pd.DataFrame(r_HA, index=col_names,columns=[''])\n",
    "print(r_HA_df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, again, the location coefficient is highly negative because of the small sample size and high proportion of visiting winners. Over a long season, this location would definitely become positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantages and Disadvantages of Massey's Method ##\n",
    "\n",
    "At its core, Massey's method is based on margin of victory. For many (such as the BCS), this is a very bad thing since what we usually care about is who wins and not by how much. Also, running up the score on a bad team should probably not improve your rating. However, people such as Jeff Sagarin note that his point based method is a better predictor than his Elo based method, which is included in the BCS ratings.\n",
    "\n",
    "Massey's method does have some more not so obvious advantages. For example, the square matrix $M$ is easy to construct and easy to study. Similarly, $p$ is also easy to construct. However, it is sort of annoying that we need to alter $M$ to get the method to work.\n",
    "\n",
    "Additionally, Massey's method automatically deals with ties and also does not require each team to play the same number of games. In other methods, these details can be a pain to deal with. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Connection Between Massey's Method and Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am now going to try to motivate Massey's Method a bit differently. At YUSAG, we use a linear regression model for our rankings. Linear regression is a classic statistical method that also relies on the method of least squares in determining the proper paramemters. Below I will attempt to show that Massey's Method is equivalent to linear regression, which offers a nice, alternative interpretation for why it works. \n",
    "\n",
    "(Note, what follows below is not exactly how the YUSAG linear regression model works. For that I would suggest [this notebook](https://github.com/mc-robinson/YUSAG_football_model/blob/master/YUSAG_FCS_football_linear_model.ipynb).)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics of Linear Regression\n",
    "\n",
    "Here, I am going to provide a very, very brief overview of how linear regression works. I suggest you consult any of the countless excellent resources on the topic if you wish to dive deeper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Form of Multiple Linear Regression\n",
    "\n",
    "In linear regression, we simply attempt to predict a scalar variable $\\hat y$ as the weighted sum of a bunch of input features (explanatory variables) and a bias term (the interecept). The general form is:\n",
    "\n",
    "$$\n",
    "\\hat y = w_0 + w_1 x_1 + w_2 x_2 + \\cdots + w_n x_n\n",
    "$$\n",
    "where:\n",
    "* $\\hat y$ is the predicted response variable\n",
    "* $x_i$ is the i'th feature value\n",
    "* $w_j$ is the j'th model parameter\n",
    "    * $w_0$ is the bias term\n",
    "    * $w_1, w_2,...,w_n$ are the feature weights\n",
    "* $n$ is the number of features\n",
    "\n",
    "So how do we figure out the values of the model coefficients $w_0,...,w_n$? The answer is that we learn these parameters when we train the linear model on the data. In fact, for linear regression, the fit is determined using the least squares criterion. That is, we seek the parameters that minimize the sum of squared errors. Once the model is trained, and the best parameters are learned, we can use the model for prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Data ####\n",
    "\n",
    "For this linear regression problem, I am going to load the data from the 2016 IV season in a specialized format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lin_reg_df = pd.read_csv('IV_linear_regression_data.csv')\n",
    "lin_reg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some explanation of this data is in order. Each row represents a game. Within each row, the feature value for the winning team is $1$ and the value for the losing team is $-1$. All the other teams receive a value of $0$. The `location` column corresponds to the location of the winning team, with $1$ being home, $-1$ being away, and $0$ being neutral. Lastly, the `MOV` column is simply the margin of victory.\n",
    "\n",
    "For example, in the game on 10/01/2016, Princeton beat Columbia at Columbia by 35 points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting Up Our Model ####\n",
    "\n",
    "Broadly, the goal of our linear regression model is to explain the margin of victory (MOV) based on the strength of the winning team and the strength of the losing team. (I am going to exclude location for now)\n",
    "\n",
    "Let's first make our training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = lin_reg_df.drop(['year','month','day','location','MOV'], axis=1)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = lin_reg_df['MOV']\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's set up the matrix equation $Xw = y$ that we are trying to solve. Remember that we must add $x_0$ and $w_0$. I am also going to set up change the weight subscripts from $1,...,n$ to the name of the schools that the weights refer to. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{bmatrix}\n",
    "  1 & 0 & 1 & 0 & -1 \\\\\n",
    "  1 & 1 & -1 & 0 & 0 \\\\\n",
    "  1 & 0 & 0 & 1 & -1 \\\\\n",
    "  1 & 1 & 0 & 0 & -1 \\\\\n",
    "  1 & 0 & 1 & -1 & 0 \\\\\n",
    "  1 & -1 & 0 & 1 & 0\n",
    "\\end{bmatrix} \n",
    "\\begin{bmatrix}\n",
    "  w_0 \\\\\n",
    "  w_h \\\\\n",
    "  w_p \\\\\n",
    "  w_y \\\\\n",
    "  w_c \n",
    "\\end{bmatrix}\n",
    "= \n",
    "\\begin{bmatrix}\n",
    "  35 \\\\\n",
    "  3 \\\\\n",
    "  8 \\\\\n",
    "  7 \\\\\n",
    "  28 \\\\\n",
    "  7\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematically, our linear model is a function of a bias term and four input features: `Harvard`, `Princeton`, `Yale`, and `Columbia`.\n",
    "\n",
    "$$\n",
    "\\tt MOV = w_0 + (w_h*{Harvard}) + (w_p*{Princeton}) + (w_y*\\tt{Yale}) + (w_c*\\tt{Columbia})\n",
    "$$\n",
    "\n",
    "Let's examine what we get from multiplying the first row of $X$ by $w$:\n",
    "\n",
    "$$\n",
    "\\tt 35 = (1*w_0) + (w_h*0) + (w_p*1) + (w_y*0) + (w_c*{-1}) \\\\\n",
    "\\tt 35 = w_0 + w_p - w_c\n",
    "$$\n",
    "\n",
    "Now, for the sake of argument, let's imagine that we set the bias term $w_0$ to be always $0$. Now the equation from the first row looks like this: \n",
    "\n",
    "$$\n",
    "\\tt 35 = w_p - w_c\n",
    "$$\n",
    "\n",
    "This looks a lot like the ideal equation from Massey's method, just with the model coefficients $w_p$ and $w_c$ replacing the ratings $r_p$ and $w_c$. That is, the difference in the model parameters for Princeton and Columbia should equal the difference in score when they play each other. If we do this for the other rows, the equations will all look similar too.\n",
    "\n",
    "Still insisting that the bias term $w_0=0$, our full linear model looks like this: \n",
    "\n",
    "$$\n",
    "\\tt MOV = (w_h*{Harvard}) + (w_p*{Princeton}) + (w_y*\\tt{Yale}) + (w_c*\\tt{Columbia})\n",
    "$$\n",
    "\n",
    "And now that we no longer care about finding $w_0$, our matrix equation $Xw = y$ looks like this:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "  0 & 1 & 0 & -1 \\\\\n",
    "  1 & -1 & 0 & 0 \\\\\n",
    "  0 & 0 & 1 & -1 \\\\\n",
    "  1 & 0 & 0 & -1 \\\\\n",
    "  0 & 1 & -1 & 0 \\\\\n",
    "  -1 & 0 & 1 & 0\n",
    "\\end{bmatrix} \n",
    "\\begin{bmatrix}\n",
    "  w_h \\\\\n",
    "  w_p \\\\\n",
    "  w_y \\\\\n",
    "  w_c\n",
    "\\end{bmatrix}\n",
    "= \n",
    "\\begin{bmatrix}\n",
    "  35 \\\\\n",
    "  3 \\\\\n",
    "  8 \\\\\n",
    "  7 \\\\\n",
    "  28 \\\\\n",
    "  7\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Look familiar? That's because this is exactly the matrix equation we got using Massey's method, just with the ratings now called weights. So this shows that Massey's method is just linear regression with the bias term set to $0$. Again, we can just solve using the method of least squares and get the desired weights, which can be interpreted as ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the Linear Regression Parameters ####\n",
    "\n",
    "As we did before, this is an overdetermined system and can be solved using the least squares method and the normal equations. I am going to train the model using the linear regression class from scikit-learn. I will also set the `fit-intercept` parameter to `False` so that the intercept is set to be $0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression(fit_intercept=False)\n",
    "lin_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print the coefficients\n",
    "print(lin_reg.intercept_)\n",
    "print(lin_reg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get the R^2 value\n",
    "r_squared = lin_reg.score(X, y)\n",
    "print('R^2 on the training data:')\n",
    "print(r_squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get the coefficients for each feature\n",
    "coef_data = list(zip(X.columns,lin_reg.coef_))\n",
    "coef_df = pd.DataFrame(coef_data,columns=['feature','feature_coef'])\n",
    "coef_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this is exactly what we got when we did the Massey ratings the old way! The coefficients of the linear regression model are precisely the ratings. And we didn't even need to do any special modifications to the matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, if we want to account for home-field advantage, all you have to do is include `location` as a feauture in the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_HA = lin_reg_df.drop(['year','month','day','MOV'], axis=1)\n",
    "X_HA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg_HA = LinearRegression(fit_intercept=False)\n",
    "lin_reg_HA.fit(X_HA, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print the coefficients\n",
    "print(lin_reg_HA.intercept_)\n",
    "print(lin_reg_HA.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get the coefficients for each feature\n",
    "coef_data = list(zip(X_HA.columns,lin_reg_HA.coef_))\n",
    "coef_df = pd.DataFrame(coef_data,columns=['feature','feature_coef'])\n",
    "coef_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, this is the same answer we got before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One other nice feature of the linear regression point of view is that we can easily adapt the model to make adjustments commonly made for linear regression. For example, if I'm afraid my model is overfitting the data, I can use [ridge regression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) to add some regularization to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "ridge_reg = Ridge(fit_intercept=False)\n",
    "ridge_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print the coefficients\n",
    "print(ridge_reg.intercept_)\n",
    "print(ridge_reg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get the R^2 value\n",
    "r_squared = ridge_reg.score(X, y)\n",
    "print('R^2 on the training data:')\n",
    "print(r_squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get the coefficients for each feature\n",
    "coef_data = list(zip(X.columns,ridge_reg.coef_))\n",
    "coef_df = pd.DataFrame(coef_data,columns=['feature','feature_coef'])\n",
    "coef_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, I can use *weighted least squares* to weight more recent games more heavily, which could be advantageous (especially if using data from past seasons). Doing this in scikit_learn is extremely easy. It's also not too difficult to implement weighting from the matrix point of view (see *Who's #1* by Langville and Meyer for details)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References ##\n",
    "\n",
    "Massey's Method:\n",
    "\n",
    "* Langville, Amy N. , and Carl D. Meyer. *Whos # 1?: The Science of Rating and Ranking* Princeton: Princeton University Press, 2012. Print.\n",
    "* Chuck Wessell's helpful [lecture notes](http://public.gettysburg.edu/~cwessell/RankingPage/massey.pdf) on the method\n",
    "\n",
    "Linear Algebra: \n",
    "* Agustinus Kristiadi's [blog post](https://wiseodd.github.io/techblog/2017/04/14/normal-equation/) on the normal equations\n",
    "* Ian Goodfellow's review of linear algebra in his [Deep Learning book](http://www.deeplearningbook.org/contents/linear_algebra.html)\n",
    "\n",
    "Linear Regression:\n",
    "* *Hands-On Machine Learning with Scikit-Learn and TensorFlow* by Aurlien Gron\n",
    "* *Learning From Data* by Abu-Mostafa et al. \n",
    "* *An Introduction to Statistical Learning* by James et al.\n",
    "* Kevin Markham's [notebook on linear regression in scikit-learn](https://github.com/justmarkham/DAT8/blob/master/notebooks/10_linear_regression.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
